"""
01_prepare_data.py
–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–±–µ–∑ —É–¥–∞—Ä–µ–Ω–∏–π ‚Üí —Å —É–¥–∞—Ä–µ–Ω–∏—è–º–∏) –≤ CSV

–®–∞–≥ 1: –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï –≤–∞–ª–∏–¥–Ω—ã–µ –ø–∞—Ä—ã –≤ CSV
–®–∞–≥ 2: –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å –∞–Ω–∞–ª–∏–∑–æ–º —Å–ª–æ–≤–∞—Ä—è
"""

import re
import csv
import json
from pathlib import Path
from typing import List, Tuple, Dict, Set
from collections import Counter
from tqdm import tqdm


class SentencePairExtractor:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ –∫–Ω–∏–≥ —Å —É–¥–∞—Ä–µ–Ω–∏—è–º–∏"""
    
    def __init__(self):
        self.stats = {
            'total_lines': 0,
            'valid_pairs': 0,
            'discarded_no_stress': 0,
            'discarded_headers': 0,
            'discarded_too_short': 0,
            'discarded_too_long': 0,
            'discarded_no_ending': 0,
        }
        
        # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.all_words = []  # –í—Å–µ —Å–ª–æ–≤–∞ (–¥–ª—è —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è)
        self.all_lemmas = set()  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Å–Ω–æ–≤—ã —Å–ª–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        
    def remove_stress_marks(self, text: str) -> str:
        """
        –£–±–∏—Ä–∞–µ—Ç —Ç–æ–ª—å–∫–æ —É–¥–∞—Ä–µ–Ω–∏—è (–∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã), —Å–æ—Ö—Ä–∞–Ω—è—è –±—É–∫–≤—É —ë
        
        Args:
            text: —Ç–µ–∫—Å—Ç —Å —É–¥–∞—Ä–µ–Ω–∏—è–º–∏ "–í—ã'–¥–∞–ª—Å—è —Ç—ë–ø–ª—ã–π"
        Returns:
            —Ç–µ–∫—Å—Ç –±–µ–∑ —É–¥–∞—Ä–µ–Ω–∏–π: "–í—ã–¥–∞–ª—Å—è —Ç—ë–ø–ª—ã–π"
        """
        # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã (–≤–∫–ª—é—á–∞—è –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞)
        text = text.replace("'", "").replace("'", "")
        return text
    
    def is_valid_sentence(self, text: str) -> Tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø–æ–ª–Ω—ã–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º
        
        Returns:
            (is_valid, reason)
        """
        text = text.strip()
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
        if len(text) < 20:
            return False, 'too_short'
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
        if len(text) > 512:
            return False, 'too_long'
        
        # –î–æ–ª–∂–Ω–æ –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
        if not text[0].isupper():
            return False, 'no_capital'
        
        # –î–æ–ª–∂–Ω–æ –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è –Ω–∞ . ! ? (–∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
        if not text[-1] in '.!?':
            return False, 'no_ending'
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –æ–±—ã—á–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–µ: "–ß–∞—Å—Ç—å I", "–ì–ª–∞–≤–∞ II"
        words = text.split()
        if len(words) < 5:
            return False, 'header'
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —É–¥–∞—Ä–µ–Ω–∏–π
        if "'" not in text and "'" not in text:
            return False, 'no_stress'
        
        return True, 'ok'
    
    def extract_words(self, text: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (—Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã)
        """
        # –£–±–∏—Ä–∞–µ–º –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        text_clean = self.remove_stress_marks(text)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ (—Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã + —ë)
        words = re.findall(r'[–∞-—è—ë–ê-–Ø–Å]+', text_clean)
        return [w.lower() for w in words]
    
    def simple_lemmatize(self, word: str) -> str:
        """
        –ü—Ä–æ—Å—Ç–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è - —É–±–∏—Ä–∞–µ—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        –ù–µ –∏–¥–µ–∞–ª—å–Ω–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        
        Args:
            word: —Å–ª–æ–≤–æ –≤ –ª—é–±–æ–π —Ñ–æ—Ä–º–µ
        Returns:
            —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Å–Ω–æ–≤–∞ —Å–ª–æ–≤–∞
        """
        word = word.lower()
        
        # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        endings = [
            '–∞–º–∏', '—è–º–∏', '–æ–≤', '–µ–≤', '–∞–º', '—è–º', '–∞—Ö', '—è—Ö',
            '–æ–º', '–µ–º', '–æ–π', '–µ–π', '–æ—é', '–µ—é', '–æ–≥–æ', '–µ–≥–æ',
            '–æ–º—É', '–µ–º—É', '—ã–º', '–∏–º', '—É—é', '—é—é', '–∞—è', '—è—è',
            '–æ–µ', '–µ–µ', '–∏–µ', '—ã–µ', '–∏—Ö', '—ã—Ö', '–∏–º–∏', '—ã–º–∏',
            '–∞', '—è', '—É', '—é', '—ã', '–∏', '–µ', '–æ'
        ]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (—Å–Ω–∞—á–∞–ª–∞ –¥–ª–∏–Ω–Ω—ã–µ)
        endings.sort(key=len, reverse=True)
        
        for ending in endings:
            if word.endswith(ending) and len(word) > len(ending) + 2:
                return word[:-len(ending)]
        
        return word
    
    def process_book_file(self, file_path: Path) -> List[Tuple[str, str]]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª –∫–Ω–∏–≥–∏
        
        Returns:
            —Å–ø–∏—Å–æ–∫ –ø–∞—Ä (source, target)
        """
        pairs = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        for line in tqdm(lines, desc=f"üìñ {file_path.name}"):
            self.stats['total_lines'] += 1
            
            line = line.strip()
            if not line:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
            is_valid, reason = self.is_valid_sentence(line)
            
            if not is_valid:
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–∏—á–∏–Ω—ã –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏—è
                if reason == 'no_stress':
                    self.stats['discarded_no_stress'] += 1
                elif reason == 'header':
                    self.stats['discarded_headers'] += 1
                elif reason == 'too_short':
                    self.stats['discarded_too_short'] += 1
                elif reason == 'too_long':
                    self.stats['discarded_too_long'] += 1
                elif reason == 'no_ending':
                    self.stats['discarded_no_ending'] += 1
                continue
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—É
            target = line  # –° —É–¥–∞—Ä–µ–Ω–∏—è–º–∏
            source = self.remove_stress_marks(target)  # –ë–µ–∑ —É–¥–∞—Ä–µ–Ω–∏–π
            
            pairs.append((source, target))
            self.stats['valid_pairs'] += 1
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            words = self.extract_words(target)
            self.all_words.extend(words)
            
            # –°–æ–±–∏—Ä–∞–µ–º –ª–µ–º–º—ã (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –æ—Å–Ω–æ–≤—ã)
            for word in words:
                lemma = self.simple_lemmatize(word)
                self.all_lemmas.add(lemma)
        
        return pairs
    
    def save_to_csv(self, pairs: List[Tuple[str, str]], output_file: Path):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ä—ã –≤ CSV —Ñ–∞–π–ª
        """
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ CSV: {output_file}")
        
        with open(output_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f, quoting=csv.QUOTE_ALL)
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            writer.writerow(['source', 'target', 'length', 'stress_count', 'word_count'])
            
            # –î–∞–Ω–Ω—ã–µ
            for source, target in tqdm(pairs, desc="–ó–∞–ø–∏—Å—å"):
                length = len(target)
                stress_count = target.count("'") + target.count("'")
                word_count = len(target.split())
                
                writer.writerow([source, target, length, stress_count, word_count])
        
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(pairs)} –ø–∞—Ä")
    
    def create_statistics(self, pairs: List[Tuple[str, str]], output_dir: Path):
        """
        –°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        """
        print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏...")
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        lengths = [len(target) for _, target in pairs]
        stress_counts = [target.count("'") + target.count("'") for _, target in pairs]
        word_counts = [len(target.split()) for _, target in pairs]
        
        # –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
        word_freq = Counter(self.all_words)
        top_words = word_freq.most_common(100000)  # –¢–æ–ø 100k —Å–ª–æ–≤
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            'extraction_stats': {
                'total_lines_processed': self.stats['total_lines'],
                'valid_pairs_extracted': self.stats['valid_pairs'],
                'discarded_total': sum([
                    self.stats['discarded_no_stress'],
                    self.stats['discarded_headers'],
                    self.stats['discarded_too_short'],
                    self.stats['discarded_too_long'],
                    self.stats['discarded_no_ending'],
                ]),
                'discarded_breakdown': {
                    'no_stress': self.stats['discarded_no_stress'],
                    'headers': self.stats['discarded_headers'],
                    'too_short': self.stats['discarded_too_short'],
                    'too_long': self.stats['discarded_too_long'],
                    'no_ending': self.stats['discarded_no_ending'],
                }
            },
            
            'sentence_stats': {
                'total_pairs': len(pairs),
                'avg_length': sum(lengths) / len(lengths) if lengths else 0,
                'min_length': min(lengths) if lengths else 0,
                'max_length': max(lengths) if lengths else 0,
                'avg_stress_marks': sum(stress_counts) / len(stress_counts) if stress_counts else 0,
                'avg_words_per_sentence': sum(word_counts) / len(word_counts) if word_counts else 0,
            },
            
            'vocabulary_stats': {
                'total_words': len(self.all_words),
                'unique_words_inflected': len(set(self.all_words)),
                'unique_words_lemmas_approx': len(self.all_lemmas),
                'top_100_words': word_freq.most_common(100),
            },
            
            'coverage': {
                'top_1000_coverage': sum([count for word, count in top_words[:1000]]) / len(self.all_words) * 100 if self.all_words else 0,
                'top_5000_coverage': sum([count for word, count in top_words[:5000]]) / len(self.all_words) * 100 if self.all_words else 0,
                'top_10000_coverage': sum([count for word, count in top_words[:10000]]) / len(self.all_words) * 100 if self.all_words else 0,
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        with open(output_dir / 'statistics.json', 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: statistics.json")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å (—Ç–æ–ø 100k)
        with open(output_dir / 'word_frequency_100k.json', 'w', encoding='utf-8') as f:
            freq_dict = {word: count for word, count in top_words}
            json.dump(freq_dict, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: word_frequency_100k.json")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Å–Ω–æ–≤ —Å–ª–æ–≤
        with open(output_dir / 'unique_lemmas.txt', 'w', encoding='utf-8') as f:
            for lemma in sorted(self.all_lemmas):
                f.write(lemma + '\n')
        
        print(f"‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Å–Ω–æ–≤—ã —Å–ª–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ): unique_lemmas.txt ({len(self.all_lemmas)} —Å–ª–æ–≤)")
        
        # –ü–µ—á–∞—Ç–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._print_summary(stats)
    
    def _print_summary(self, stats: Dict):
        """–ü–µ—á–∞—Ç–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É"""
        print("\n" + "=" * 70)
        print("üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø")
        print("=" * 70)
        
        print(f"\nüìö –û–±—Ä–∞–±–æ—Ç–∫–∞:")
        print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['extraction_stats']['total_lines_processed']:,}")
        print(f"   ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –ø–∞—Ä: {stats['extraction_stats']['valid_pairs_extracted']:,}")
        print(f"   ‚ùå –û—Ç–±—Ä–æ—à–µ–Ω–æ: {stats['extraction_stats']['discarded_total']:,}")
        
        print(f"\nüìä –†–∞–∑–±–∏–≤–∫–∞ –æ—Ç–±—Ä–æ—à–µ–Ω–Ω—ã—Ö:")
        for reason, count in stats['extraction_stats']['discarded_breakdown'].items():
            print(f"   - {reason}: {count:,}")
        
        print(f"\nüìè –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:")
        print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {stats['sentence_stats']['avg_length']:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –î–∏–∞–ø–∞–∑–æ–Ω: {stats['sentence_stats']['min_length']} - {stats['sentence_stats']['max_length']}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ —É–¥–∞—Ä–µ–Ω–∏–π: {stats['sentence_stats']['avg_stress_marks']:.1f}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ —Å–ª–æ–≤: {stats['sentence_stats']['avg_words_per_sentence']:.1f}")
        
        print(f"\nüìñ –°–ª–æ–≤–∞—Ä—å:")
        print(f"   –í—Å–µ–≥–æ —Å–ª–æ–≤–æ—É–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–π: {stats['vocabulary_stats']['total_words']:,}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ (—Å –æ–∫–æ–Ω—á–∞–Ω–∏—è–º–∏): {stats['vocabulary_stats']['unique_words_inflected']:,}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ (–æ—Å–Ω–æ–≤—ã, ~): {stats['vocabulary_stats']['unique_words_lemmas_approx']:,}")
        
        print(f"\nüéØ –ü–æ–∫—Ä—ã—Ç–∏–µ —á–∞—Å—Ç–æ—Ç–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º:")
        print(f"   –¢–æ–ø-1000 —Å–ª–æ–≤: {stats['coverage']['top_1000_coverage']:.1f}%")
        print(f"   –¢–æ–ø-5000 —Å–ª–æ–≤: {stats['coverage']['top_5000_coverage']:.1f}%")
        print(f"   –¢–æ–ø-10000 —Å–ª–æ–≤: {stats['coverage']['top_10000_coverage']:.1f}%")
        
        print(f"\nüî§ –¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤:")
        for i, (word, count) in enumerate(stats['vocabulary_stats']['top_100_words'][:20], 1):
            print(f"   {i:2d}. {word:15s} - {count:,} —Ä–∞–∑")


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """
    print("=" * 70)
    print("üìö –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–ê–† –ü–†–ï–î–õ–û–ñ–ï–ù–ò–ô –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò –£–î–ê–†–ï–ù–ò–ô")
    print("=" * 70)
    print("\n–®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∞—Ä –≤ CSV")
    print("–®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    BOOKS_DIR = Path("./books")
    OUTPUT_DIR = Path("./temp")
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–Ω–∏–≥
    if not BOOKS_DIR.exists():
        print(f"‚ùå –û—à–∏–±–∫–∞: –ø–∞–ø–∫–∞ {BOOKS_DIR} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        print(f"   –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É './books' –∏ –ø–æ–ª–æ–∂–∏—Ç–µ —Ç—É–¥–∞ .txt —Ñ–∞–π–ª—ã")
        return
    
    book_files = list(BOOKS_DIR.glob("*.txt"))
    if not book_files:
        print(f"‚ùå –û—à–∏–±–∫–∞: –≤ {BOOKS_DIR} –Ω–µ—Ç .txt —Ñ–∞–π–ª–æ–≤!")
        return
    
    print(f"üìñ –ù–∞–π–¥–µ–Ω–æ –∫–Ω–∏–≥: {len(book_files)}")
    for i, book in enumerate(book_files[:10], 1):
        print(f"   {i:2d}. {book.name}")
    if len(book_files) > 10:
        print(f"   ... –∏ –µ—â–µ {len(book_files) - 10} –∫–Ω–∏–≥")
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
    print("\n" + "-" * 70)
    extractor = SentencePairExtractor()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –∫–Ω–∏–≥–∏
    all_pairs = []
    
    for book_file in book_files:
        pairs = extractor.process_book_file(book_file)
        all_pairs.extend(pairs)
    
    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–∞—Ä: {len(all_pairs):,}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    csv_file = OUTPUT_DIR / "sentence_pairs.csv"
    extractor.save_to_csv(all_pairs, csv_file)
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    extractor.create_statistics(all_pairs, OUTPUT_DIR)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã
    samples_file = OUTPUT_DIR / "samples.txt"
    print(f"\nüìù –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤: {samples_file}")
    with open(samples_file, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("–ü–†–ò–ú–ï–†–´ –ò–ó–í–õ–ï–ß–ï–ù–ù–´–• –ü–ê–†\n")
        f.write("=" * 70 + "\n\n")
        
        for i, (source, target) in enumerate(all_pairs[:50], 1):
            stress_count = target.count("'") + target.count("'")
            f.write(f"–ü—Ä–∏–º–µ—Ä {i}:\n")
            f.write(f"  –í—Ö–æ–¥:  {source}\n")
            f.write(f"  –í—ã—Ö–æ–¥: {target}\n")
            f.write(f"  –î–ª–∏–Ω–∞: {len(target)} —Å–∏–º–≤–æ–ª–æ–≤, —É–¥–∞—Ä–µ–Ω–∏–π: {stress_count}\n\n")
    
    print(f"‚úÖ –ü—Ä–∏–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã (–ø–µ—Ä–≤—ã–µ 50)")
    
    print("\n" + "=" * 70)
    print("‚úÖ –ì–û–¢–û–í–û!")
    print("=" * 70)
    print(f"\nüìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {OUTPUT_DIR}/")
    print(f"   üìÑ sentence_pairs.csv - –≤—Å–µ –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
    print(f"   üìä statistics.json - –¥–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    print(f"   üìñ word_frequency_100k.json - —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å (100k —Å–ª–æ–≤)")
    print(f"   üìù unique_lemmas.txt - —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Å–Ω–æ–≤—ã —Å–ª–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)")
    print(f"   üìù samples.txt - –ø—Ä–∏–º–µ—Ä—ã (–ø–µ—Ä–≤—ã–µ 50 –ø–∞—Ä)")
    print("\nüéØ –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: –∞–Ω–∞–ª–∏–∑ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è\n")


if __name__ == "__main__":
    main()